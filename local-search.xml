<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>变分自编码器</title>
    <link href="/2025/03/28/Variational-Autoencoder.html"/>
    <url>/2025/03/28/Variational-Autoencoder.html</url>
    
    <content type="html"><![CDATA[<h6id="说明本文是对变分自编码器的学习总结">说明：本文是对变分自编码器的学习总结</h6><span id="more"></span><p>论文链接：<a href="https://arxiv.org/abs/1312.6114">🔗</a>本人的关于论文的粗略翻译<a href="">🔗</a></p><h2 id="当我们谈论-vae-时我们在谈论什么">1. 当我们谈论 VAE时，我们在谈论什么？</h2><p>变分自编码器（VariationalAutoencoder，VAE）是一种生成模型，它通过学习数据的潜在表示来生成新的数据样本。VAE的主要目标是找到一个潜在变量的分布，使得原始数据可以被表示为这个分布的概率分布。</p><p><img src="/assets/images/vae.png" /></p><p>通常的 VAE模型包括一个编码器（Encoder）和一个解码器（Decoder）。而在无监督方法中，我们仅需要编码器，而不需要解码器。</p><p>接下来让我们 dive into the details of VAE。</p><h2 id="我的一点看法">2. 我的一点看法</h2><h3id="要讲vae首先看看vae以前的时代">要讲VAE,首先看看VAE以前的时代。</h3><p>最初AE（Autoencoder）由两个主要组件组成：编码器<spanclass="math inline">\(\mathbf{z}=f_{\phi}(\mathbf{x})\)</span>和解码器<spanclass="math inline">\(\mathbf{x}=g_{\theta}(\mathbf{z})\)</span>。编码器的目标是将输入数据<spanclass="math inline">\(\mathbf{x}\)</span>映射到一个低维的潜在表示<spanclass="math inline">\(\mathbf{z}\)</span>，而解码器的目标是将潜在表示<spanclass="math inline">\(\mathbf{z}\)</span>映射回原始数据空间<spanclass="math inline">\(\mathbf{x}\)</span>。AE通常使用的Loss函数是<spanclass="math inline">\(\ell=\|X-\tilde{X}\|^2\)</span>。这个Loss函数能够很直观的告诉我们重构出的数据和原始数据之间的差异。</p><h2 id="一些参考资料">3. 一些参考资料</h2><p><ahref="https://zhuanlan.zhihu.com/p/348498294">机器学习方法—优雅的模型（一）：变分自编码器（VAE）</a></p><p><ahref="https://www.bilibili.com/video/BV1aE411o7qd/?p=170">机器学习-白板推导系列-变分自编码器</a></p><p><ahref="https://zhuanlan.zhihu.com/p/389295612">如何避免VAE后验坍塌?</a></p><p><ahref="https://spaces.ac.cn/archives/5253">科学空间-苏剑林-变分自编码器（一）：原来是这么一回事</a></p><p><ahref="https://spaces.ac.cn/archives/5343">科学空间-苏剑林-变分自编码器（二）：从贝叶斯观点出发</a></p><p><ahref="https://spaces.ac.cn/archives/5383">科学空间-苏剑林-变分自编码器（三）：这样做为什么能成？</a></p><p><ahref="https://spaces.ac.cn/archives/5887">科学空间-苏剑林-变分自编码器（四）：一步到位的聚类方案</a></p><p><ahref="https://spaces.ac.cn/archives/7381">科学空间-苏剑林-变分自编码器（五）：VAE+ BN = 更好的VAE</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>math</tag>
      
      <tag>unsupervised learning</tag>
      
      <tag>VAE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人工智能基础</title>
    <link href="/2025/03/09/The-way-to-Artificial-Intelligence.html"/>
    <url>/2025/03/09/The-way-to-Artificial-Intelligence.html</url>
    
    <content type="html"><![CDATA[<h6id="说明介绍目前的深度学习需要学习什么以及一些比较好的资源">说明：介绍目前的深度学习需要学习什么，以及一些比较好的资源。</h6><span id="more"></span><blockquote><h2 id="基础技术栈">基础技术栈</h2><ul><li><strong>系统</strong>：Debian<br /></li><li><strong>环境管理</strong>：venv，pip<br /></li><li><strong>编程语言</strong>：Python<br /></li><li><strong>AI 框架</strong>：Pytorch<br /></li><li><strong>内容管理工具</strong>：Git<br /></li><li><strong>集成开发环境</strong>：VScode（本地环境）、Jupyterlab（远程环境）</li></ul></blockquote><blockquote><h2 id="进阶技术栈">进阶技术栈</h2><ul><li><strong>编程语言</strong>：C++（g++、cling，标准=20）<br /></li><li><strong>AI 框架</strong>：Pytorch（AMP，DDP并发）<br /></li><li><strong>环境管理</strong>：Docker（开发容器与普通容器）<br /></li><li><strong>高性能计算</strong>：jax<br /></li><li><strong>大数据</strong>：duckdb（替代pandas）<br /></li><li><strong>构建工具</strong>：xmake</li></ul></blockquote><blockquote><h2 id="自选方向技术栈">自选方向技术栈</h2><ul><li><strong>数学建模</strong>：MATLAB / Mworks（Julia）<br /></li><li><strong>大数据</strong>：pyspark<br /></li><li><strong>科学计算</strong>：计算方法，Julia<br /></li><li><strong>云计算</strong>：K8s（rancher）、KVM、SDS（ceph）、SDN<br /></li><li><strong>边缘智能</strong>：Tensort、onnx<br /></li><li><strong>高性能AI</strong>：flax、optax、orbax</li><li><strong>加速器</strong>：FPGA（xilinx）、vitis、PCIe</li></ul></blockquote><blockquote><p><strong>注：</strong><br />1. 基础技术栈建议在进入方向后3-6个月内完成；<br />2. 进阶技术栈建议在基础学习完后1-2个月内完成；<br />3. 自选方向技术栈仅作为参考，目前阶段无强制要求；<br />4. 使用“/”连接表示可选择其中一种技术，使用“，”仅作为分隔符；<br />5. WSL2不再推荐，仅作为过渡使用。</p></blockquote><hr /><h3 id="以下推荐一些需要的数学知识">以下推荐一些需要的数学知识</h3><ul><li><strong><ahref="https://www.zhihu.com/column/gs-linear-algebra">线性代数</a></strong>：介绍了人工智能所需的线性代数基础</li><li><strong><ahref="https://www.zhihu.com/column/c_1334301979816820736">机器学习方法</a></strong>：深入介绍机器学习中的一些基础方法</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>简单介绍一下我本人的环境搭建</title>
    <link href="/2024/09/29/My-environment-setup.html"/>
    <url>/2024/09/29/My-environment-setup.html</url>
    
    <content type="html"><![CDATA[<h6id="说明仅介绍本人主要使用的配置留档以便后来可快速配置">说明：仅介绍本人主要使用的配置，留档以便后来可快速配置。</h6><span id="more"></span><h3 id="目录">目录</h3><ul><li><a href="#step-0-系统启动盘制作">Step 0. 系统启动盘制作</a></li><li><a href="#step-1-终端美化与代理设置">Step 1.终端美化与代理设置</a></li><li><a href="#step-2-安装NVIDIA驱动">Step 2. 安装NVIDIA驱动</a></li><li><a href="#step-3-安装python环境管理工具">Step 3.安装python环境管理工具</a></li></ul><h2 id="step-0.-系统启动盘制作">Step 0. 系统启动盘制作</h2><p>简单来说，Ventoy是一个制作可启动U盘的开源工具。有了Ventoy你就无需反复地格式化U盘，你只需要把ISO/WIM/IMG/VHD(x)/EFI等类型的文件直接拷贝到U盘里面就可以启动了，无需其他操作。你可以一次性拷贝很多个不同类型的镜像文件，Ventoy会在启动时显示一个菜单来供你进行选择。 下载地址:<ahref="https://www.ventoy.net/cn/download.html"class="uri">https://www.ventoy.net/cn/download.html</a>安装到U盘以后，只需要将iso镜像复制到U盘中即可。个人建议前往镜像站下载操作系统的iso镜像。</p><p>安装过程因人而异，故此不再赘述。个人比较喜欢Debian，在安装进行到分区这一步时，建议删除除引导分区以外的其它分区，我们不需要swap，以及这样做方便我们创建Btrfs主分区，将它的挂载点设置在“/”下。完成安装。(建议使用DVD镜像，避免安装时过多的等待)</p><p>接着进入系统，切换为root用户，为自己创建的用户添加sudo权限，并进行换源。此处不再赘述。值得一提的是，换源不再建议使用tuna(清华)源(用的人太多了，容易断流)。</p><p>至此，我们的系统就初步配置好了。</p><h2 id="step-1.-终端美化与代理设置">Step 1. 终端美化与代理设置</h2><p>终端个人推荐使用zsh，主题使用powerlevel10k，插件仅需zsh-autosuggestions与zsh-syntax-highlighting即可。配置可参照:<a href="https://www.haoyep.com/posts/zsh-config-oh-my-zsh/"class="uri">https://www.haoyep.com/posts/zsh-config-oh-my-zsh/</a></p><p>代理的设置建议如下 新建<strong>~/scripts/proxy.sh</strong>,并在该脚本文件中复制以下代码,其中hostip和port按需更改:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/sh</span><br>hostip=127.0.0.1<br>port=7890<br><br>PROXY_HTTP=<span class="hljs-string">&quot;http://<span class="hljs-variable">$&#123;hostip&#125;</span>:<span class="hljs-variable">$&#123;port&#125;</span>&quot;</span><br><br><span class="hljs-function"><span class="hljs-title">set_proxy</span></span>()&#123;<br>  <span class="hljs-built_in">export</span> http_proxy=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;PROXY_HTTP&#125;</span>&quot;</span><br>  <span class="hljs-built_in">export</span> HTTP_PROXY=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;PROXY_HTTP&#125;</span>&quot;</span><br><br>  <span class="hljs-built_in">export</span> https_proxy=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;PROXY_HTTP&#125;</span>&quot;</span><br>  <span class="hljs-built_in">export</span> HTTPS_proxy=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;PROXY_HTTP&#125;</span>&quot;</span><br><br>  <span class="hljs-built_in">export</span> ALL_PROXY=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;PROXY_SOCKS5&#125;</span>&quot;</span><br>  <span class="hljs-built_in">export</span> all_proxy=<span class="hljs-variable">$&#123;PROXY_SOCKS5&#125;</span><br><br>  git config --global http.https://github.com.proxy <span class="hljs-variable">$&#123;PROXY_HTTP&#125;</span><br>  git config --global https.https://github.com.proxy <span class="hljs-variable">$&#123;PROXY_HTTP&#125;</span><br><br>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Proxy has been opened.&quot;</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-title">unset_proxy</span></span>()&#123;<br>  <span class="hljs-built_in">unset</span> http_proxy<br>  <span class="hljs-built_in">unset</span> HTTP_PROXY<br>  <span class="hljs-built_in">unset</span> https_proxy<br>  <span class="hljs-built_in">unset</span> HTTPS_PROXY<br>  <span class="hljs-built_in">unset</span> ALL_PROXY<br>  <span class="hljs-built_in">unset</span> all_proxy<br>  git config --global --<span class="hljs-built_in">unset</span> http.https://github.com.proxy<br>  git config --global --<span class="hljs-built_in">unset</span> https.https://github.com.proxy<br><br>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Proxy has been closed.&quot;</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-title">test_setting</span></span>()&#123;<br>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Host IP:&quot;</span> <span class="hljs-variable">$&#123;hostip&#125;</span><br>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Try to connect to Google...&quot;</span><br>  resp=$(curl -I -s --connect-timeout 5 -m 5 -w <span class="hljs-string">&quot;%&#123;http_code&#125;&quot;</span> -o /dev/null www.google.com)<br>  <span class="hljs-keyword">if</span> [ <span class="hljs-variable">$&#123;resp&#125;</span> = 200 ]; <span class="hljs-keyword">then</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Proxy setup succeeded!&quot;</span><br>  <span class="hljs-keyword">else</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Proxy setup failed!&quot;</span><br>  <span class="hljs-keyword">fi</span><br>&#125;<br><br><span class="hljs-keyword">if</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$1</span>&quot;</span> = <span class="hljs-string">&quot;set&quot;</span> ]<br><span class="hljs-keyword">then</span><br>  set_proxy<br><br><span class="hljs-keyword">elif</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$1</span>&quot;</span> = <span class="hljs-string">&quot;unset&quot;</span> ]<br><span class="hljs-keyword">then</span><br>  unset_proxy<br><br><span class="hljs-keyword">elif</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$1</span>&quot;</span> = <span class="hljs-string">&quot;test&quot;</span> ]<br><span class="hljs-keyword">then</span><br>  test_setting<br><span class="hljs-keyword">else</span><br>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Unsupported arguments.&quot;</span><br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure> 在你的.zshrc或者.bashrc中添加<strong>alias proxy=“source~/scripts/proxy.sh”</strong></p><p>使用时只需要在终端输入proxy set;proxy unset;proxy test.</p><p>至此,终端美化与代理设置就初步完成了</p><h2 id="step-2.-安装nvidia驱动">Step 2. 安装NVIDIA驱动</h2><p>你完全可以<strong>sudo apt installnvidia-driver</strong>来安装开源驱动但是我更推荐安装闭源驱动，如果有内核更新，记得要<strong>sudo apt installlinux-headers-$(uname -r)</strong>.</p><p>安装 apt GPG keyring 包，目的是获取 GPG 密钥 <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">wget</span> https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/cuda-keyring_1.<span class="hljs-number">1</span>-<span class="hljs-number">1</span>_all.deb<br><span class="hljs-attribute">sudo</span> dpkg -i cuda-keyring_1.<span class="hljs-number">1</span>-<span class="hljs-number">1</span>_all.deb<br></code></pre></td></tr></table></figure></p><p>可以到此位置<ahref="https://developer.download.nvidia.com/compute/cuda/repos/"class="uri">https://developer.download.nvidia.com/compute/cuda/repos/</a>浏览具体发行版</p><p>apt 安装驱动 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt update<br><span class="hljs-built_in">sudo</span> apt -y install nvidia-driver cuda-drivers<br></code></pre></td></tr></table></figure></p><p>至此,你已经几乎完成了环境的搭建。</p><h2 id="step-3.-安装python环境管理工具">Step 3.安装python环境管理工具</h2><p>这个看个人品味,我推荐使用miniconda,venv或者mamba.镜像站使用tuna或者bfsu.在此笔者假设读者熟悉上面三种的任意一种创建完虚拟的环境后,进入虚拟环境对pip进行换源.直接运行 <figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> torch<br></code></pre></td></tr></table></figure>如果在安装完torch后安装了大量nvidia*的包,那么就可以放心了,你安装的是pytorchwith GPU</p><h5id="你已经完成了深度学习环境搭建立刻开始愉快的学习吧">你已经完成了深度学习环境搭建,立刻开始愉快的学习吧！</h5>]]></content>
    
    
    
    <tags>
      
      <tag>env setup</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Intel A770 GPU深度学习环境搭建（Linux）</title>
    <link href="/2024/06/28/A770_deeplearning_env_setup.html"/>
    <url>/2024/06/28/A770_deeplearning_env_setup.html</url>
    
    <content type="html"><![CDATA[<h6id="说明windows下也可以进行环境搭建但是个人更偏爱debian">说明：Windows下也可以进行环境搭建，但是个人更偏爱Debian。</h6><span id="more"></span><h6id="updatepytorch在24年10月中旬发布了2.5版本添加了intel-gpu支持使用以下代码以启用."><strong>Update:pytorch在24年10月中旬发布了2.5版本,添加了IntelGPU支持,使用以下代码以启用.</strong></h6><p>这是pytorch官方的链接<ahref="https://pytorch.org/docs/main/notes/get_start_xpu.html"class="uri">https://pytorch.org/docs/main/notes/get_start_xpu.html</a></p><p>我们需要准备好GPU驱动,oneAPI.(不过现在仍然处于Preview版本)<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">pip3 install torch torchvision torchaudio --index-url https:<span class="hljs-regexp">//</span>download.pytorch.org<span class="hljs-regexp">/whl/</span>test/xpu<br></code></pre></td></tr></table></figure></p><h6id="以下外国博客也可参考我仍然只推荐使用torch2.5preview"><strong>以下外国博客也可参考,我仍然只推荐使用torch2.5preview</strong></h6><p><ahref="https://christianjmills.com/posts/intel-pytorch-extension-tutorial/native-ubuntu/"class="uri">https://christianjmills.com/posts/intel-pytorch-extension-tutorial/native-ubuntu/</a></p><h3 id="目录">目录</h3><ul><li><a href="#step-0-准备工作">Step 0. 准备工作</a></li><li><a href="#step-1-安装GPU驱动">Step 1. 安装GPU驱动</a></li><li><a href="#step-2-安装Intel®-oneAPI-Base-Toolkit">Step 2.安装Intel®-oneAPI-Base-Toolkit</a></li><li><a href="#step-3-安装Intel®-Extension-for-PyTorch">Step 3.安装Intel®-Extension-forPyTorch</a></li><li><a href="#step-4-安装xpu-smi">Step 4. 安装xpu-smi</a></li></ul><h2 id="step-0-准备工作">Step 0 准备工作</h2><p>首先你需要有一台使用Intel GPU的电脑，本文针对Intel ArcA770（16GB）编写，系统环境为Debian sid。</p><p>关于Intel® Extension for PyTorch的安装可以参考官方示例<ahref="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu&amp;version=v2.1.30%2bxpu&amp;os=linux%2fwsl2&amp;package=pip"class="uri">https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu&amp;version=v2.1.30%2bxpu&amp;os=linux%2fwsl2&amp;package=pip</a>。以下部分是我自己踩的一点坑。</p><h2 id="step-1-安装gpu驱动">step 1 安装GPU驱动</h2><p>前往<ahref="https://dgpu-docs.intel.com/driver/client/overview.html">IntelGPU驱动网站</a>，进行其中的3.1.1-3.1.5部分。树外内核部分无需理会。</p><h2 id="step-2-安装intel-oneapi-base-toolkit">step 2安装Intel®-oneAPI-Base-Toolkit</h2><p>前往<ahref="https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html?operatingsystem=linux&amp;linux-install-type=apt">Intel®-oneAPI-Base-Toolkit</a>网站，使用apt进行安装。</p><h2 id="step-3-安装intel-extension-forpytorch">step 3安装Intel®-Extension-forPyTorch</h2><p>我选择的是使用pip进行安装。你可以先使用conda创建一个虚拟环境，python版本推荐3.11。安装命令 <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">python</span> -m pip install torch==<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.post2 torchvision==<span class="hljs-number">0</span>.<span class="hljs-number">16</span>.<span class="hljs-number">0</span>.post2 torchaudio==<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.post2 intel-extension-for-pytorch==<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">30</span>.post0 oneccl_bind_pt==<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">300</span>+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/<br></code></pre></td></tr></table></figure></p><p>完成后进行测试。</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gradle"><span class="hljs-keyword">source</span> <span class="hljs-regexp">/opt/i</span>ntel<span class="hljs-regexp">/oneapi/</span>setvars.sh<br></code></pre></td></tr></table></figure><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss">python -c &quot;import torch; import intel_extension_for_pytorch as ipex; <span class="hljs-built_in">print</span>(torch.__version__); <span class="hljs-built_in">print</span>(ipex.__version__); <span class="hljs-selector-attr">[print(f<span class="hljs-string">&#x27;[&#123;i&#125;]: &#123;torch.xpu.get_device_properties(i)&#125;&#x27;</span>) for i in range(torch.xpu.device_count())]</span>;&quot;<br></code></pre></td></tr></table></figure><p>最后成功识别到显卡即为安装成功。</p><p>前往<ahref="https://intel.github.io/intel-extension-for-pytorch/xpu/2.1.30+xpu/tutorials/examples.html">example</a>进行愉快的玩耍吧。</p><h1 id="step-4-安装xpu-smi">step 4 安装xpu-smi</h1><p>最后一部分我也还在摸索。使用这个软件也是因为intel-gpu-tools查看不了显存使用情况。</p><p>前往<ahref="https://github.com/intel/xpumanager/releases/tag/V1.2.37">release</a>页面下载安装即可。</p><p>使用说明在GitHub仓库的readme中。</p>]]></content>
    
    
    
    <tags>
      
      <tag>env setup</tag>
      
      <tag>intel GPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
