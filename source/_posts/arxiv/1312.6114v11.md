---
title: "Auto-Encoding Variational Bayes"
tags: [AI, math, VAE, unsupervised learning]
banner_img: /assets/images/vae-banner.png
archive: true
math: true
---

## Introduction

论文的 introduction 部分我们略过，直接看 method 部分。

## 2. Method

本节中的策略可用于为具有连续潜在变量(隐变量)的各种有向图形模型推导下限估计器（随机目标函数）。我们在这里将自己限制在常见情况下：

- 我们有一个 i.i.d. 数据集，每个数据点都有潜在变量。
- 我们希望对（全局）参数执行最大似然（ML）或最大后验（MAP）推理，以及对潜在变量进行变分推理。

这里的潜在变量是什么就很值得玩味了。

### 2.1 问题场景(Problem Scenario)

让我们考虑一些数据集$\mathbf{X} = \{\mathbf{x}^{(i)}\}_{i=1}^N$，它由一些连续或离散变量$x$的$N$个i.i.d.样本组成。

我们假设数据是由某个随机过程生成的，涉及一个未观察到的连续随机变量$z$。该过程包括以下两个步骤：

1.从某个先验分布$p_{\boldsymbol{\theta}^*}(\mathbf{z})$生成一个值$\mathbf{z}^{(i)}$  
2.值$\mathbf{x}^{(i)}$是从某个条件分布$p_{\boldsymbol{\theta}^*}(\mathbf{x}|\mathbf{z})$生成的

我们假设先验$p_{\boldsymbol{\theta}^* }(\mathbf{z})$和似然$p_{\boldsymbol{\theta}^*}(\mathbf{x}|\mathbf{z})$来自分布$p_{\boldsymbol{\theta}}(\mathbf{z})$和$p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})$的参数族，并且它们的概率密度函数（PDF）几乎在$\boldsymbol{\theta}$和$\mathbf{z}$的任何地方都是可微分的。

#### 隐藏的挑战

然而，不幸的是，这个过程的很多内容都隐藏在我们看不见的地方：

- 我们不知道真实参数$\boldsymbol{\theta}^*$。
- 我们也不知道潜在变量$\mathbf{z}^{(i)}$的值。

非常重要的是，我们没有对边际或后验概率做出常见的简化假设。相反，我们在这里对一种通用算法感兴趣，该算法甚至可以在以下情况下有效工作：

#### 难解性

- **边际似然难以处理**：  
  边际似然$p_{\boldsymbol{\theta}}(\mathbf{x}) = \int p_{\boldsymbol{\theta}}(\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})d\mathbf{z}$的积分难以处理，因此我们无法评估或区分边际似然。

- **真实后验密度难以处理**：  
  真实后验密度$$p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}) = \frac{p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{z})}{p_{\boldsymbol{\theta}}(\mathbf{x})}$$难以处理，因此不能使用期望最大算法。

- **积分复杂性**：  
  任何合理的均场变分贝叶斯推断算法所需的积分也难以处理。这些难解性很常见，出现在中等复杂似然函数$p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})$的情况下，例如具有非线性隐藏层的神经网络。

#### 数据集规模

- **数据集过大**：  
  数据太多，批量优化成本太高。我们希望使用小批量甚至单个数据点进行参数更新。

- **基于采样的解决方案过慢**：  
  基于采样的解决方案（例如蒙特卡罗期望最大算法）通常太慢，因为它通常涉及每个数据点昂贵的采样循环。

#### 相关问题与解决方案

我们对上述场景中的三个相关问题感兴趣，并提出了解决方案：

1. **参数$\boldsymbol{\theta}$的有效近似最大似然估计或最大后验估计估计**：  
   参数本身可能很有趣，例如，如果我们正在分析一些自然过程。它们还允许我们模拟隐藏的随机过程并生成类似于真实数据的人工数据。

2. **潜在变量$\mathbf{z}$的有效近似后验推断**：  
   对于参数$\boldsymbol{\theta}$的选择，给定观测值$\mathbf{x}$，潜在变量$\mathbf{z}$的有效近似后验推断。这对于编码或数据表示任务非常有用。

3. **变量$\mathbf{x}$的有效近似边际推理**：  
   这使我们能够执行需要通过先验$\mathbf{x}$的各种推理任务。计算机视觉中的常见应用包括图像去噪、修复和超分辨率。
   
为了解决上述问题，让我们引入一个识别模型$q_{\phi}(z|x)$：这是对难以处理的真实后验分布$p_{\theta}(z|x)$的一种近似。需要注意的是，与均值场变分推断中的近似后验不同，该模型不要求具有因子分解形式，其参数$\phi$也不是通过闭式期望计算得到的。相反，我们将提出一种方法，**使识别模型参数$\phi$能够与生成模型参数$\theta$被联合学习**。

从编码理论的视角来看，未观测变量$z$可以解释为潜在表示或编码。因此，在本文中，我们将识别模型$q_{\phi}(z|x)$称为概率**编码器**——当给定数据点$x$时，它会生成一个关于编码$z$可能取值的分布（例如高斯分布），该编码能够生成数据点$x$。类似地，我们将$p_{\theta}(x|z)$称为概率**解码器**——当给定编码$z$时，它会生成一个关于对应数据点$x$可能取值的分布。

### 2.2 变分界(The variational bound)

边际似然由各个数据点的边际似然之和构成$\log p_{\theta}(x^{(1)}, \cdots, x^{(N)}) = \sum_{i=1}^{N} \log p_{\theta}(x^{(i)})$，其中每个项均可重写为：
$$\begin{align} & 
    \log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})=D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}
    (\mathbf{z}|\mathbf{x}^{(i)}))+\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\tag{1}
\end{align}$$

第一项的右侧是近似后验分布与真实后验分布之间的KL散度。由于该KL散度非负，第二项的右侧$\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})$被称为数据点 $i$ 边缘似然的（变分）下界，其表达式可表示为：
$$
    \log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})\geq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})}\left[-\log q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})+\log p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})\right]\tag{2}
$$

也可以写成如下形式：
$$
    \mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right]\tag{3}
$$

我们需要对下界 $\mathcal{L}(\theta, \phi; \mathbf{x}^{(i)})$ 关于变分参数 $\phi$ 和生成参数 $\theta$ 进行微分和优化。然而，下界关于 $\phi$ 的梯度计算存在一定问题。针对此类问题的常规（朴素）蒙特卡洛梯度估计器为：  

$$
    \nabla_{\boldsymbol{\phi}}\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z})}\left[f(\mathbf{z})\right]=\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z})}\left[f(\mathbf{z})\nabla_{q_{\boldsymbol{\phi}}(\mathbf{z})}\log q_{\boldsymbol{\phi}}(\mathbf{z})\right]\simeq\frac{1}{L}\sum_{l=1}^{L}f(\mathbf{z})\nabla_{q_{\boldsymbol{\phi}}(\mathbf{z}^{(l)})}\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(l)})
$$

其中 $\mathbf{z}^{(l)} \sim q_\phi(\mathbf{z} | \mathbf{x}^{(i)})$。  

该梯度估计器的方差极高，因此在实际应用中难以有效使用。  

### 2.3 SGVB估计器（随机梯度变分贝叶斯估计器，Stochastic Gradient Variational Bayes 和 AEVB算法（自动编码变分贝叶斯，Auto-Encoding Variational Bayes 

本节我们将介绍一种针对变分下界及其参数导数的实用估计方法。我们采用条件近似后验分布$q_\phi(\mathbf{z} | \mathbf{x})$的形式，但需要注意的是，该技术同样适用于非条件形式$q_\phi(\mathbf{z})$（即不依赖于x的情况）。

在满足第2.4节所述的特定温和条件下，对于选定的近似后验分布$q_\phi(\mathbf{z} | \mathbf{x})$，我们可以通过一个可微变换$g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})$对随机变量$\widetilde{\mathbf{z}}\sim q_{\phi}(\mathbf{z}|\mathbf{x})$进行重新参数化，其中$\epsilon$是一个辅助噪声变量：
$$
    \widetilde{\mathbf{z}}=g_\phi(\boldsymbol{\epsilon},\mathbf{x})\quad\mathrm{with~}\quad\boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})\tag{4}
$$

关于如何选择合适的分布 $p(\boldsymbol{\epsilon})$ 和函数 $g_\phi(\boldsymbol{\epsilon},\mathbf{x})$ 的一般性策略，请参阅第 2.4 节的内容。基于此，我们可以按以下方式构建关于 $q_{\phi}(\mathbf{z}|\mathbf{x})$ 的期望函数 $f(z)$ 的蒙特卡洛估计：
$$
    \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[f(\mathbf{z})\right]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}^{(i)}))\right]\simeq\frac{1}{L}\sum_{l=1}^{L}f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)},\mathbf{x}^{(i)}))\quad\mathrm{where}\quad\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})\tag{5}
$$

我们运用这一技术处理变分下界（公式(2)），由此得到通用的随机梯度变分贝叶斯（SGVB）估计量 $\widetilde{\mathcal{L}}^A(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})$，其表达式为：
$$\begin{aligned}
 & \widetilde{\mathcal{L}}^A(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=\frac{1}{L}\sum_{l=1}^L\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)},\mathbf{z}^{(i,l)})-\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(i,l)}|\mathbf{x}^{(i)}) \\
 & \mathrm{where}\quad\mathbf{z}^{(i,l)}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\quad\mathrm{and}\quad\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})
\end{aligned}\tag{6}
$$


通常，公式(3)中的KL散度$D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))$可以解析计算，因此只需通过采样估计期望重构误差$\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right]$。此时，KL散度项可解释为对$\phi$的正则化项，促使近似后验分布接近先验分布$p_{\boldsymbol{\theta}}(\mathbf{z})$。由此可以得到SGVB估计量的第二个版本$\widetilde{\mathcal{L}}^B(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})$，对应公式(3)，该估计量通常比通用估计量具有更小的方差：
$$\begin{aligned}
 & \widetilde{\mathcal{L}}^B(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\frac{1}{L}\sum_{l=1}^L(\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)})) \\
 & \mathrm{where}\quad\mathbf{z}^{(i,l)}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\quad\mathrm{and}\quad\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})
\end{aligned}\tag{7}
$$

对于包含$N$个数据的数据集$X$，在给定多个数据的情况下，我们可以基于小批量数据构建完整数据集边际似然下界的估计量：
$$\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X})\simeq\widetilde{\mathcal{L}}^M(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}^M)=\frac{N}{M}\sum_{i=1}^M\widetilde{\mathcal{L}}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\tag{8}
$$ 其中，小批量数据$\mathbf{X}^M=\{\mathbf{x}^{(i)}\}_{i=1}^M$是从包含$N$个数据的完整数据集$X$中随机抽取的$M$个数据点子集。实验结果表明，只要小批量规模$M$足够大（例如$M$=100），每个数据的样本数$L$可设为1。我们可以计算梯度$\nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\widetilde{\mathcal{L}}(\boldsymbol{\theta};\mathbf{X}^M)$，并将所得梯度与随机优化方法（如SGD或Adagrad）结合使用。随机梯度的基本计算方法请参见**Algorithm 1**。

![](/assets/images/vae-algorithm.png)

当我们分析公式(7)给出的目标函数时，其与自编码器的关联就变得清晰可见。其中，第一项（近似后验分布与先验分布的KL散度）充当正则化项，而第二项则是期望负重构误差。函数$g_{\phi}(.)$的设计使得它能将数据点$x^{(i)}$和随机噪声向量$\epsilon^{(l)}$映射为该数据点的近似后验分布样本：$\mathbf{z}^{(i,l)}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)},\mathbf{x}^{(i)})$，其中$\mathbf{z}^{(i,l)}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})$。随后，样本$z^{(i,l)}$被输入函数$\log p_\theta(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)})$，该函数表示在生成模型中给定$\mathbf{z}^{(i,l)}$时数据点$\mathbf{x}^{(i)}$的概率密度（或质量）。用自编码器的术语来说，这一项就是负*重构误差*。

### 2.4 重参数化技巧

为解决这一问题，我们采用了一种从条件分布$q_\phi(\mathbf{z}|\mathbf{x})$中生成样本的替代方法。这一核心的重参数化技巧原理其实非常简单：假设 $\mathbf{z}$ 是一个连续随机变量，且$\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})$为某个条件分布。此时通常可以将随机变量 $\mathbf{z}$ 重新表达为一个确定性变量：$z = g_\phi(\epsilon,\mathbf{x})$，其中 $\epsilon$ 是一个具有独立边界$p(\epsilon)$的辅助变量，并且$g_\phi(.)$是由$\phi$参数化的某个向量值函数。

该重参数化方法之所以适用于我们的情况，是因为它能将关于$q_\phi(\mathbf{z}|\mathbf{x})$的期望改写成可对$\phi$求导的形式，从而使得蒙特卡洛估计对$\phi$可微。具体证明如下：给定确定性映射关系 $\mathbf{z} = g_\phi(\epsilon,\mathbf{x})$，根据概率密度变换关系可得：
$q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})\prod_idz_i=p(\boldsymbol{\epsilon})\prod_id\epsilon_i$
因此(请注意，对于无穷小，我们使用 $d\mathbf{z}=\prod_idz_i$)，期望可被重写为：
$$
\int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})f(\mathbf{z}) d\mathbf{z} = \int p(\boldsymbol{\epsilon})f(\mathbf{z}) d\boldsymbol{\epsilon} = \int p(\boldsymbol{\epsilon})f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})) d\boldsymbol{\epsilon}
$$

由此可得，我们可以构建一个可微估计量：
$$
\int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})f(\mathbf{z}) d\mathbf{z} \simeq \frac{1}{L} \sum_{l=1}^L f(g_{\boldsymbol{\phi}}(\boldsymbol{\mathbf{x},\epsilon}^{(l)})) 
$$ 其中，$\boldsymbol{\epsilon}^{(l)} \sim p(\epsilon)$。在2.3节中，我们应用这一技巧得到了变分下界的可微估计量。

以单变量高斯分布为例：设 $\mathbf{z}\sim p(\mathbf{z}|\mathbf{x})=\mathcal{N}(\mu,\sigma^{2})$。此时，一个有效的重参数化形式为 $z=\mu+\sigma\epsilon$，其中 $\epsilon$ 是辅助噪声变量 $\epsilon\sim\mathcal{N}(0,1)$。因此，其期望可表示为：
$$
\begin{array}{r}{\mathbb{E}_{\mathcal{N}(z;\mu,\sigma^{2})}\left[f(z)\right]=\mathbb{E}_{\mathcal{N}(\epsilon;0,1)}\left[f(\mu+\sigma\epsilon)\right]\simeq\frac{1}{L}\sum_{l=1}^{L}f(\mu+\sigma\epsilon^{(l)})}\end{array}
$$ 其中 $\epsilon^{(l)}\sim\mathcal{N}(0,1)$。

对于条件分布 $q_{\phi}(\mathbf{z}|\mathbf{x})$，我们可以选择这样的可微变换 $g_{\phi}(\cdot)$ 和辅助变量 $\epsilon\sim p(\epsilon)$吗？主要有以下三种基本方法：

1. **可逆 累积分布函数法（CDF）**：若 $q_{\phi}(\mathbf{z}|\mathbf{x})$ 存在易处理的逆CDF，设 $\epsilon\sim\mathcal{U}(\mathbf{0},\mathbf{I})$，令 $g_{\phi}(\epsilon,\mathbf{x})$ 为该分布的逆CDF。  
   适用分布：指数分布、柯西分布、逻辑斯谛分布、瑞利分布、帕累托分布、威布尔分布、倒数分布、冈珀茨分布、冈贝尔分布和埃尔朗分布。

2. **位置-尺度分布族通用解法**：  
   类比高斯分布案例，对于任何"位置-尺度"族分布，均可按以下方式构造重参数化：  
   设标准分布（位置参数 $\mathit{\theta}=0$，尺度参数 $\lambda=1$）为辅助变量 $\epsilon$ 定义变换函数 $g(\cdot) = \mu + \sigma \cdot \epsilon$ 其中 $\mu$ 为位置参数，$\sigma$ 为尺度参数  
   适用分布：拉普拉斯分布、椭圆分布、学生t分布、逻辑斯谛分布、均匀分布、三角分布和高斯分布。

3. **组合变换法**：通过辅助变量的复合变换实现，常见形式包括：  
   对数正态分布（正态分布变量的指数变换），伽马分布（多个指数分布变量的和），狄利克雷分布（伽马变量的加权和），贝塔分布、卡方分布、F分布等

当上述三种方法均不适用时，仍可通过计算复杂度与概率密度函数（PDF）相当的数值方法来获得逆累积分布函数（inverse CDF）的高精度近似解。

至此，总算到了我们关心的部分。

## 3. 例子：变分自编码器

在本节中，我们将给出一个应用示例：使用神经网络构建概率编码器 $q_{\phi}(\mathbf{z}|\mathbf{x})$（作为生成模型 $p_{\pmb{\theta}}(\mathbf{x},\mathbf{z})$ 后验分布的近似），并通过AEVB算法联合优化参数 $\phi$ 和 $\pmb{\theta}$。

设隐变量的先验分布为中心各向同性多元高斯分布 $p_{\pmb{\theta}}(\mathbf{z}) = \mathcal{N}(\mathbf{z};\mathbf{0},{\mathbf{I}})$。需要注意的是，这种情况下先验分布不含参数。我们令 $p_{\pmb{\theta}}(\mathbf{x}|\mathbf{z})$ 为多元高斯分布（实值数据）或伯努利分布（二值数据），其分布参数通过一个MLP从 $\mathbf{z}$ 计算得到。需要注意的是，真实后验 $p_{\pmb{\theta}}(\mathbf{z}|\mathbf{x})$ 在这种情况下是难以处理的。

虽然 $q_{\phi}(\mathbf{z}|\mathbf{x})$ 的形式选择具有很大自由度，但我们假设真实（但难以处理的）后验服从近似高斯形式且具有近似对角协方差矩阵。在这种情况下，我们可以设变分近似后验为具有对角协方差结构的多元高斯分布（请注意，这只是一个（简化的）选择，而不是我们方法的限制）：
$$
\log q_{\phi}(\mathbf{z}|\mathbf{x}^{(i)})=\log\mathcal{N}(\mathbf{z};\pmb{\mu}^{(i)},\pmb{\sigma}^{2(i)}\mathbf{I})\tag{9}
$$
如第2.4节所述，我们使用$$
\mathbf{z}^{(i,l)} = g_{\phi}(\mathbf{x}^{(i)},\epsilon^{(l)}) = \pmb{\mu}^{(i)} + \pmb{\sigma}^{(i)} \odot \pmb{\epsilon}^{(l)}
$$从后验分布$\mathbf{z}^{(i,l)}\sim q_{\phi}(\mathbf{z}|\mathbf{x}^{(i)})$中采样，其中 $\epsilon^{(l)} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$，符号 $\odot$ 表示逐元素乘积。在该模型中：先验分布 $p_{\pmb{\theta}}(\mathbf{z})$ 与近似后验 $q_{\phi}(\mathbf{z}|\mathbf{x})$ 均为高斯分布,可直接计算KL散度（无需估计）并求导,基于公式(7)的估计量，对数据 $\mathbf{x}^{(i)}$ 的最终估计式为：
$$
\begin{array}{r l}&{\mathcal{L}(\pmb{\theta},\phi;\mathbf{x}^{(i)})\simeq\displaystyle\frac{1}{2}\sum_{j=1}^{J}\left(1+\log((\sigma_{j}^{(i)})^{2})-(\mu_{j}^{(i)})^{2}-(\sigma_{j}^{(i)})^{2}\right)+\frac{1}{L}\sum_{l=1}^{L}\log p_{\pmb{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)})}\\ &{\mathrm{where}\quad\mathbf{z}^{(i,l)}=\pmb{\mu}^{(i)}+\pmb{\sigma}^{(i)}\odot\pmb{\epsilon}^{(l)}\quad\mathrm{and}\quad\pmb{\epsilon}^{(l)}\sim\mathcal{N}(0,\mathbf{I})}\end{array}\tag{10}
$$  
如上所述，解码项$\log p_{\pmb{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)})$根据建模数据类型的不同，可表现为伯努利MLP或高斯MLP的形式。

余下部分就不详细展开了。